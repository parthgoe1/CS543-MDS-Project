{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3060cfa1-23d5-466b-8e4a-ca860515c56f",
   "metadata": {},
   "source": [
    "# MDS Project\n",
    "# Extracting Semantics from Billion Edge Graphs\n",
    "# Author: Parth Goel (pg514)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd11e887-3520-480a-9011-c89c63cfcfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "from pyspark.sql.types import StructType,StructField, IntegerType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col\n",
    "import networkx as nx\n",
    "import ast\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6307cf5b-97f1-4374-b86a-c7076f52f0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f191ca57820>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"Project_group5\").getOrCreate()\n",
    "spark.sparkContext.getConf().setAll([('spark.executor.memory', '40g'), ('spark.cores.max', '20'), ('spark.driver.memory','40g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "103f21eb-87f7-4749-88a2-48b749fffd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa261a7-1b06-457b-85ca-bdb8c6f8b7c5",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1eef1b-9055-460b-80e0-cd4ef8a5c9da",
   "metadata": {},
   "source": [
    "### 1.1 Reading the movies_labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01008cfa-0155-4e49-a4b3-349d61618e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the movie_label.csv\n",
    "movies_label = spark.read.csv(\"movies_label.csv\")\n",
    "movies_label = movies_label.toDF('vertex_id', 'vertex_tag_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd04fc9-4744-433e-b461-7c75f05ee9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|vertex_id|   vertex_tag_label|\n",
      "+---------+-------------------+\n",
      "|    17827|            officer|\n",
      "|    18716|       conscription|\n",
      "|   188853|    naked in public|\n",
      "|    22078|      public nudity|\n",
      "|    43001|   male rear nudity|\n",
      "|    20676|        male nudity|\n",
      "|    21844|male frontal nudity|\n",
      "|    12417|            soldier|\n",
      "|    11870|               army|\n",
      "|   317967|            f rated|\n",
      "|    28047|         voice mail|\n",
      "|    15119|      disappearance|\n",
      "|    13676|            monster|\n",
      "|   180654| man versus monster|\n",
      "|    17434|           nobleman|\n",
      "|    23220|             switch|\n",
      "|    82211|          role swap|\n",
      "|    12753|      role reversal|\n",
      "|   317991|     egocentric man|\n",
      "|   290367|        selfish man|\n",
      "+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying the movies labels\n",
    "# _c0: vertex id; _c1: vertex tag label\n",
    "movies_label.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f14f78-328c-47b5-813c-f41546066816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218269"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the count of movies_label\n",
    "movies_label.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170f3cf-cef2-455c-85e3-8599cf656ac8",
   "metadata": {},
   "source": [
    "### 1.2 Getting all the files in the Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38f1194f-0e64-40b0-bd60-8faf5022691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of all the files in the data folder\n",
    "path = \"Data\"\n",
    "file_names = os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6207e3e-db98-477f-b0b5-da632b4b9dcd",
   "metadata": {},
   "source": [
    "### 1.3 Finding all the distinct layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "693ec0a9-9792-4f77-a4b4-73aed29bbb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Layers:  {'56', '35', '19', '21', '44', '17', '18', '42', '6', '5', '9', '47', '26', '29', '101', '13', '37', '27', '53', '527', '8', '2', '185', '84', '39', '119', '88', '34', '24', '12', '16', '10', '31', '38', '3114', '3', '22', '61', '283', '123', '62', '66', '51', '87', '55', '58', '33', '50', '1', '30', '76', '54', '23', '52', '4', '57', '15', '25', '60', '11', '1246', '28', '98', '7', '20', '14', '32', '43', '414', '48', '36', '40', '757', '41', '86', '2070', '105', '46'}\n",
      "no of Distinct Layers:  78\n"
     ]
    }
   ],
   "source": [
    "# Finding all the distinct layers in the dataset\n",
    "layers = []\n",
    "for files in file_names:\n",
    "    layers.append(files.split(\"-\")[1])\n",
    "distinctLayers = set(layers)\n",
    "print(\"Distinct Layers: \", distinctLayers)\n",
    "print(\"no of Distinct Layers: \", len(distinctLayers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dbd63a-1bfc-4520-b680-109b653fd792",
   "metadata": {},
   "source": [
    "### 1.4 Iterating through all the layers one by one to read wcc-lcc mapping files and the edges files and making the edges-lcc connection and saving it in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa67ab13-6a6d-42f4-8b51-100e8472c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through all the layers\n",
    "for layer in distinctLayers:\n",
    "    # Iniitializing an empty RDD\n",
    "    emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "    # Creating the schema for the empty Dataframe\n",
    "    schema = StructType([\n",
    "      StructField('source', IntegerType(), True),\n",
    "      StructField('target', IntegerType(), True),\n",
    "      StructField('waveIndex', IntegerType(), True),\n",
    "      StructField('wcc', IntegerType(), True),\n",
    "      StructField('fragmentIndex', IntegerType(), True),\n",
    "      ])\n",
    "\n",
    "    # Initializing the empty Dataframe using the empty RDD and schema\n",
    "    merged_edges = spark.createDataFrame(emptyRDD, schema)\n",
    "    # merged_edges.printSchema()\n",
    "    \n",
    "    # Iterating through the files\n",
    "    for files in file_names:\n",
    "        # Checking if it is the correct layer\n",
    "        if files.startswith(\"layer-\" + layer + \"-\"):\n",
    "            # Checking if it is the wcc-lcc mapping file\n",
    "            if files.endswith('.wcc-lcc'):\n",
    "                wcc_lcc_mapping = spark.read.csv(\"Data/\" + files)\n",
    "                wcc_lcc_mapping = wcc_lcc_mapping.toDF('wcc', 'lcc')\n",
    "            # Checking if it is the edges file\n",
    "            elif files.endswith('.csv'):\n",
    "                edges = spark.read.csv(\"Data/\" + files)\n",
    "                edges = edges.toDF('source', 'target', 'waveIndex', 'wcc', 'fragmentIndex')\n",
    "                merged_edges = merged_edges.union(edges)\n",
    "    \n",
    "    # Adding a lcc column in the merged edges dataframe\n",
    "    merged_edges.createOrReplaceTempView(\"merged_edges\")\n",
    "    wcc_lcc_mapping.createOrReplaceTempView(\"wcc_lcc_mapping\")\n",
    "    edges_lcc_mapping = spark.sql(\"select merged_edges.source, merged_edges.target, merged_edges.waveIndex, merged_edges.wcc, merged_edges.fragmentIndex, wcc_lcc_mapping.lcc from merged_edges left join wcc_lcc_mapping on merged_edges.wcc = wcc_lcc_mapping.wcc\")\n",
    "\n",
    "    # Finding the degrees of all the vertices\n",
    "    temp_edges = edges_lcc_mapping.drop(\"target\", \"waveIndex\", \"wcc\", \"fragmentIndex\", \"lcc\")\n",
    "    df_degree = temp_edges.groupBy('source').count()\n",
    "    df_degree = df_degree.toDF(\"vertex\", \"count\")\n",
    "    \n",
    "    # Saving the Degree of the vertices\n",
    "    df_degree.coalesce(1).write.options(header='True', delimiter=',').csv(\"Degrees/layer-\" + layer)\n",
    "    \n",
    "    # Removing Duplicates\n",
    "    # Adding a sum and product column to find duplicates\n",
    "    edges_lcc_mapping = edges_lcc_mapping.withColumn(\"sum\", col(\"source\") + col(\"target\"))\n",
    "    edges_lcc_mapping = edges_lcc_mapping.withColumn(\"product\", col(\"source\") * col(\"target\"))\n",
    "    \n",
    "    # Dropping the rows in which sum and product is same to get every edge once\n",
    "    edges_lcc_mapping = edges_lcc_mapping.dropDuplicates(['sum', 'product'])\n",
    "    \n",
    "    # Dropping the sum and product columns as we dont need it anymore\n",
    "    edges_lcc_mapping = edges_lcc_mapping.drop(\"sum\", \"product\")\n",
    "    \n",
    "    # Saving the undirected graph edges\n",
    "    edges_lcc_mapping.coalesce(1).write.options(header='True', delimiter=',').csv(\"Undirected Edges/layer-\" + layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ade73f2-69cc-4ecb-89e6-e18a9c18adce",
   "metadata": {},
   "source": [
    "### 1.5 Saving the edges layer wise in files but this time connecting the vertices both ways instead of using degrees to find direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d55d951-9abc-413e-a33f-d9d1edec9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through all the layers\n",
    "for layer in distinctLayers:\n",
    "    # Iniitializing an empty RDD\n",
    "    emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "    # Creating the schema for the empty Dataframe\n",
    "    schema = StructType([\n",
    "      StructField('source', IntegerType(), True),\n",
    "      StructField('target', IntegerType(), True),\n",
    "      StructField('waveIndex', IntegerType(), True),\n",
    "      StructField('wcc', IntegerType(), True),\n",
    "      StructField('fragmentIndex', IntegerType(), True),\n",
    "      ])\n",
    "\n",
    "    # Initializing the empty Dataframe using the empty RDD and schema\n",
    "    merged_edges = spark.createDataFrame(emptyRDD, schema)\n",
    "    # merged_edges.printSchema()\n",
    "    \n",
    "    # Iterating through the files\n",
    "    for files in file_names:\n",
    "        # Checking if it is the correct layer\n",
    "        if files.startswith(\"layer-\" + layer + \"-\"):\n",
    "            # Checking if it is the wcc-lcc mapping file\n",
    "            if files.endswith('.wcc-lcc'):\n",
    "                wcc_lcc_mapping = spark.read.csv(\"Data/\" + files)\n",
    "                wcc_lcc_mapping = wcc_lcc_mapping.toDF('wcc', 'lcc')\n",
    "            # Checking if it is the edges file\n",
    "            elif files.endswith('.csv'):\n",
    "                edges = spark.read.csv(\"Data/\" + files)\n",
    "                edges = edges.toDF('source', 'target', 'waveIndex', 'wcc', 'fragmentIndex')\n",
    "                merged_edges = merged_edges.union(edges)\n",
    "    \n",
    "    # Adding a lcc column in the merged edges dataframe\n",
    "    merged_edges.createOrReplaceTempView(\"merged_edges\")\n",
    "    wcc_lcc_mapping.createOrReplaceTempView(\"wcc_lcc_mapping\")\n",
    "    edges_lcc_mapping = spark.sql(\"select merged_edges.source, merged_edges.target, merged_edges.waveIndex, merged_edges.wcc, merged_edges.fragmentIndex, wcc_lcc_mapping.lcc from merged_edges left join wcc_lcc_mapping on merged_edges.wcc = wcc_lcc_mapping.wcc\")\n",
    "    \n",
    "    # Saving the undirected graph edges\n",
    "    edges_lcc_mapping.coalesce(1).write.options(header='True', delimiter=',').csv(\"Undirected Edges Two-way/layer-\" + layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0cc49f-ef2e-4751-bf7d-7c8df6122c8f",
   "metadata": {},
   "source": [
    "### 1.6 Making directed edges and saving to files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1455e08-3516-4847-af79-31b8018f5386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping edges to RDD from dataframe\n",
    "def mapper(x):\n",
    "    source = x.source\n",
    "    target = x.target\n",
    "    waveIndex = x.waveIndex\n",
    "    wcc = x.wcc\n",
    "    fragmentIndex = x.fragmentIndex\n",
    "    lcc = x.lcc\n",
    "    return (source, target, waveIndex, wcc, fragmentIndex, lcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbd34a9c-05f0-4277-b0f8-5753e394fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directing edges depending on number of neighbours\n",
    "def directEdges(x, degree_dict):\n",
    "    source = int(x[0])\n",
    "    target = int(x[1])\n",
    "    waveIndex = int(x[2])\n",
    "    wcc = int(x[3])\n",
    "    fragmentIndex = int(x[4])\n",
    "    lcc = int(x[5])\n",
    "    if degree_dict[source] <= degree_dict[target]:\n",
    "        return (source, target, waveIndex, wcc, fragmentIndex, lcc)\n",
    "    else:\n",
    "        return (target, source, waveIndex, wcc, fragmentIndex, lcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4182a79-5986-4794-844c-9f80fde978ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the undirected graph to directed graph using number of neighbours\n",
    "for layer in distinctLayers:\n",
    "    degree_path = \"Degrees/layer-\" + layer\n",
    "    undirected_edges_path = \"Undirected Edges/layer-\" + layer\n",
    "    \n",
    "    degree_file_names = os.listdir(degree_path)\n",
    "    undirected_edges_file_names = os.listdir(undirected_edges_path)\n",
    "    \n",
    "    for file in degree_file_names:\n",
    "        if file.endswith(\".csv\"):\n",
    "            degree = spark.read.options(header='True').csv(degree_path + \"/\" + file)\n",
    "        \n",
    "    for file in undirected_edges_file_names:\n",
    "        if file.endswith(\".csv\"):\n",
    "            undirected_edges = spark.read.options(header='True').csv(undirected_edges_path + \"/\" + file)\n",
    "    \n",
    "    # Mapping all the edges\n",
    "    edges = undirected_edges.rdd.map(lambda x: mapper(x))\n",
    "    \n",
    "    # Converting number of neighbours dataframe to dictionary\n",
    "    degree_dict = degree.rdd.map(lambda x: (int(x[\"vertex\"]), int(x[\"count\"]))).collectAsMap()\n",
    "    \n",
    "    # Directing edges depending on degree\n",
    "    directed_edges_rdd = edges.map(lambda x: directEdges(x, degree_dict))\n",
    "    \n",
    "    # Converting rdd to dataframe\n",
    "    directed_edges_df = directed_edges_rdd.toDF(['source', 'target', 'waveIndex', 'wcc', 'fragmentIndex', 'lcc'])\n",
    "    \n",
    "    # Saving the undirected graph edges\n",
    "    directed_edges_df.coalesce(1).write.options(header='True', delimiter=',').csv(\"Directed Edges/layer-\" + layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b05d9-51cc-4590-9ba6-1918ded8788c",
   "metadata": {},
   "source": [
    "### 1.7 Saving the edges buildings wise in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "128232cd-1dab-4782-8b00-044b2c24039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through directed edges layer file to store it building wise\n",
    "for layer in distinctLayers:\n",
    "    directed_edge_path = \"Directed Edges/layer-\" + layer\n",
    "    directed_edges_file_names = os.listdir(directed_edge_path)\n",
    "    \n",
    "    for file in directed_edges_file_names:\n",
    "        if file.endswith(\".csv\"):\n",
    "            directed_edges = spark.read.options(header='True').csv(directed_edge_path + \"/\" + file)\n",
    "    \n",
    "    # Finding the distinct lcc values (i.e., buildings)\n",
    "    buildings = directed_edges.select('lcc').distinct().collect()\n",
    "    \n",
    "    # Iterating through buildings to find all the edges\n",
    "    for building in buildings:\n",
    "        # Making a SQL query to find all the rows of the same lcc\n",
    "        directed_edges.createOrReplaceTempView(\"de\")\n",
    "        building_edges = spark.sql(\"select * from de where lcc = {}\".format(building[0]))\n",
    "        \n",
    "        # Saving the undirected graph edges per building\n",
    "        building_edges.coalesce(1).write.mode('append').options(header='True', delimiter=',').csv(\"Building Edges/building-\" + building[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039734e2-a80f-4090-bfe9-93875d8bf908",
   "metadata": {},
   "source": [
    "### 1.8 Saving the edges building wise in files with Two-way connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b64acde2-8f37-4403-9757-74cfa9e6334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through undirected edges two-way layer file to store it building wise\n",
    "for layer in distinctLayers:\n",
    "    undirected_edge_path = \"Undirected Edges Two-way/layer-\" + layer\n",
    "    undirected_edges_file_names = os.listdir(undirected_edge_path)\n",
    "    \n",
    "    for file in undirected_edges_file_names:\n",
    "        if file.endswith(\".csv\"):\n",
    "            undirected_edges = spark.read.options(header='True').csv(undirected_edge_path + \"/\" + file)\n",
    "\n",
    "    # Finding the distinct lcc values (i.e., buildings)\n",
    "    buildings = undirected_edges.select('lcc').distinct().collect()\n",
    "    \n",
    "    # Iterating through buildings to find all the edges\n",
    "    for building in buildings:\n",
    "        # Making a SQL query to find all the rows of the same lcc\n",
    "        undirected_edges.createOrReplaceTempView(\"ude\")\n",
    "        building_edges = spark.sql(\"select * from ude where lcc = {}\".format(building[0]))\n",
    "        \n",
    "        # Saving the undirected graph edges per building\n",
    "        building_edges.coalesce(1).write.mode('append').options(header='True', delimiter=',').csv(\"Building Edges Two-way/building-\" + building[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128ca74-eb44-4193-a477-3cd69ca3107f",
   "metadata": {},
   "source": [
    "### 1.9 Finding distinct vertices in buildings and saving it along with their tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6390fd9-c996-48c4-a72e-d123ac40eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through undirected edges two-way layer file to store it building wise\n",
    "for layer in distinctLayers:\n",
    "    undirected_edge_path = \"Undirected Edges Two-way/layer-\" + layer\n",
    "    undirected_edges_file_names = os.listdir(undirected_edge_path)\n",
    "    \n",
    "    for file in undirected_edges_file_names:\n",
    "        if file.endswith(\".csv\"):\n",
    "            undirected_edges = spark.read.options(header='True').csv(undirected_edge_path + \"/\" + file)\n",
    "            \n",
    "    # Dropping the uneccessary columns\n",
    "    undirected_edges = undirected_edges.drop(\"target\", \"waveIndex\", \"wcc\", \"fragmentIndex\")\n",
    "    \n",
    "    # Dropping rows where the source and the lcc are same to get every vertex only once\n",
    "    distinct_vertices = undirected_edges.dropDuplicates(['source', 'lcc'])\n",
    "    \n",
    "    # Mapping all the vertices with their tag names from movies_label.csv\n",
    "    movies_label.createOrReplaceTempView(\"movies_label\")\n",
    "    distinct_vertices.createOrReplaceTempView(\"distinct_vertices\")\n",
    "    vertex_tags_mapping = spark.sql(\"select distinct_vertices.source as vertex, distinct_vertices.lcc, movies_label.vertex_tag_label from distinct_vertices left join movies_label on distinct_vertices.source = movies_label.vertex_id\")\n",
    "    \n",
    "    # Finding the distinct lcc values (i.e., buildings)\n",
    "    buildings = vertex_tags_mapping.select('lcc').distinct().collect()\n",
    "    \n",
    "    vertex_tags_mapping.createOrReplaceTempView(\"vtm\")\n",
    "    \n",
    "    # Iterating through buildings to find all the edges\n",
    "    for building in buildings:\n",
    "        # Making a SQL query to find all the rows of the same lcc\n",
    "        building_tags = spark.sql(\"select * from vtm where lcc = {}\".format(building[0]))\n",
    "        \n",
    "        # Saving the undirected graph edges per building\n",
    "        building_tags.coalesce(1).write.mode('append').options(header='True', delimiter=',').csv(\"Building Tags/building-\" + building[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28f326-f754-4be3-868f-09fc620f9d55",
   "metadata": {},
   "source": [
    "## 2. Page Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471a0a1-9cf8-4b30-ab0a-e8d9ccc561b3",
   "metadata": {},
   "source": [
    "### 2.1 Iterating over all the buildings and implementing Page Rank on directed edges and saving in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c4418b2-727e-4351-9aa5-e0f93ad4d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping edges to RDD from dataframe\n",
    "def edge_mapper(x):\n",
    "    u = x.source\n",
    "    v = x.target\n",
    "    return (int(u), int(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6c489b-1ae9-4b4c-8efe-c4a7d9e1f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of all the files in the Building Edges folder\n",
    "be_path = \"Building Edges\"\n",
    "be_file_names = os.listdir(be_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6bddefaf-ef8f-4c3f-8785-282216df9368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over building files\n",
    "for file in be_file_names:\n",
    "    building_edges_file_names = os.listdir(\"Building Edges/\" + file)\n",
    "\n",
    "    for csv_file in building_edges_file_names:\n",
    "        if csv_file.endswith(\".csv\"):\n",
    "            building_edges = spark.read.options(header='True').csv(\"Building Edges/\" + file + \"/\" + csv_file)\n",
    "    \n",
    "    # Dropping uneccessary columns\n",
    "    building_edges = building_edges.drop(\"waveIndex\", \"wcc\", \"fragmentIndex\", \"lcc\")\n",
    "    \n",
    "    # Converting df to rdd returning source and target\n",
    "    building_edges_rdd = building_edges.rdd.map(lambda x: edge_mapper(x))\n",
    "    \n",
    "    # Converting rdd to list\n",
    "    building_edges_list = building_edges_rdd.collect()\n",
    "    \n",
    "    # Building a Directed Graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Adding all the edges to the Graph\n",
    "    G.add_edges_from(building_edges_list)\n",
    "    \n",
    "    # Implementing Page Rank\n",
    "    pr = nx.pagerank(G)\n",
    "    \n",
    "    # Converting pr to list to sort it\n",
    "    pr_list = []\n",
    "    for key, value in pr.items():\n",
    "        pr_list.append([key, value])\n",
    "    \n",
    "    # Sorting the Page Rank \n",
    "    sorted_pr_list = sorted(pr_list, key=lambda x: (x[1], x[0]))\n",
    "    sorted_pr_list.reverse()\n",
    "    \n",
    "    # Writing the sorted page rank in a file\n",
    "    with open('Page Rank/' + file, 'w') as f:\n",
    "          \n",
    "        # using csv.writer method from CSV package\n",
    "        write = csv.writer(f)\n",
    "\n",
    "        write.writerow(['vertexID', 'pageRank'])\n",
    "        write.writerows(sorted_pr_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4466e9b3-eaba-4b88-a1c4-bdabd4ecb7b6",
   "metadata": {},
   "source": [
    "### 2.2 Iterating over all the buildings and implementing Page Rank on Two-way edges and saving in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64fcfe8c-f440-428d-b89e-3d03b1f190db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of all the files in the Building Edges Two-way folder\n",
    "bet_path = \"Building Edges Two-way\"\n",
    "bet_file_names = os.listdir(bet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c74e4b4-49b9-4d86-acba-c21aeb9a8819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over building files\n",
    "for file in bet_file_names:\n",
    "    building_edges_file_names = os.listdir(\"Building Edges Two-way/\" + file)\n",
    "\n",
    "    for csv_file in building_edges_file_names:\n",
    "        if csv_file.endswith(\".csv\"):\n",
    "            building_edges = spark.read.options(header='True').csv(\"Building Edges Two-way/\" + file + \"/\" + csv_file)\n",
    "    \n",
    "    # Dropping uneccessary columns\n",
    "    building_edges = building_edges.drop(\"waveIndex\", \"wcc\", \"fragmentIndex\", \"lcc\")\n",
    "    \n",
    "    # Converting df to rdd returning source and target\n",
    "    building_edges_rdd = building_edges.rdd.map(lambda x: edge_mapper(x))\n",
    "    \n",
    "    # Converting rdd to list\n",
    "    building_edges_list = building_edges_rdd.collect()\n",
    "    \n",
    "    # Building a Directed Graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Adding all the edges to the Graph\n",
    "    G.add_edges_from(building_edges_list)\n",
    "    \n",
    "    # Implementing Page Rank\n",
    "    pr = nx.pagerank(G)\n",
    "    \n",
    "    # Converting pr to list to sort it\n",
    "    pr_list = []\n",
    "    for key, value in pr.items():\n",
    "        pr_list.append([key, value])\n",
    "    \n",
    "    # Sorting the Page Rank \n",
    "    sorted_pr_list = sorted(pr_list, key=lambda x: (x[1], x[0]))\n",
    "    sorted_pr_list.reverse()\n",
    "    \n",
    "    # Writing the sorted page rank in a file\n",
    "    with open('Page Rank Two-way/' + file, 'w') as f:\n",
    "          \n",
    "        # using csv.writer method from CSV package\n",
    "        write = csv.writer(f)\n",
    "\n",
    "        write.writerow(['vertexID', 'pageRank'])\n",
    "        write.writerows(sorted_pr_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739a770b-4141-469c-a145-16b7bf47f3f8",
   "metadata": {},
   "source": [
    "## 3. Streaming Kmeans Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7635a-ca4a-40db-b484-3d753b6624fc",
   "metadata": {},
   "source": [
    "### 3.1 Installing and importing the library to return optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d949f038-a6a6-4d53-a903-09514175c3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gap-stat in /common/home/rb1182/.local/lib/python3.9/site-packages (2.0.1)\n",
      "Requirement already satisfied: numpy in /usr/lib/anaconda3/lib/python3.9/site-packages (from gap-stat) (1.21.5)\n",
      "Requirement already satisfied: pandas in /usr/lib/anaconda3/lib/python3.9/site-packages (from gap-stat) (1.4.2)\n",
      "Requirement already satisfied: scipy in /usr/lib/anaconda3/lib/python3.9/site-packages (from gap-stat) (1.7.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/lib/anaconda3/lib/python3.9/site-packages (from pandas->gap-stat) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/anaconda3/lib/python3.9/site-packages (from pandas->gap-stat) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->gap-stat) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install --user gap-stat\n",
    "from gap_statistic import OptimalK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ffdbdc-db16-4565-9f36-491eac4ac125",
   "metadata": {},
   "source": [
    "### 3.2 Iterating over all the building tag files to implement Kmeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56503c23-5d34-4c0d-ba14-428e8a85f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the text vectorization library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ef64e52-06e7-4f54-9da3-09f495983218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initializing spark context\n",
    "# sc = SparkContext.getOrCreate()\n",
    "# ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abffb276-3760-4587-b2ee-ca63ede9fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of all the files in the Building Tags folder\n",
    "bt_path = \"Building Tags\"\n",
    "bt_file_names = os.listdir(bt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce0cd5d4-6e1c-44f5-b147-889c95d25dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping tags to RDD from dataframe\n",
    "def tags_mapper(x):\n",
    "    u = x.vertex_tag_label\n",
    "    return (u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "335df0e1-8b27-4a4d-b064-b42d36c3bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterating over all the building tag files to implement Streaming Kmeans Clustering\n",
    "# for file in bt_file_names:\n",
    "#     building_tags_file_names = os.listdir(\"Building Tags/\" + file)\n",
    "\n",
    "#     for csv_file in building_tags_file_names:\n",
    "#         if csv_file.endswith(\".csv\"):\n",
    "#             building_tags = spark.read.options(header='True').csv(\"Building Tags/\" + file + \"/\" + csv_file)\n",
    "            \n",
    "#     # Dropping the lcc column as we don't need it now\n",
    "#     building_tags = building_tags.drop(\"lcc\")\n",
    "    \n",
    "#     # Converting df to rdd returning vertex_tag_label\n",
    "#     building_tags_rdd = building_tags.rdd.map(lambda x: tags_mapper(x))\n",
    "\n",
    "#     # Converting rdd to list\n",
    "#     building_tags_list = building_tags_rdd.collect()\n",
    "    \n",
    "#     # Vectorizing the tags\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     X = vectorizer.fit_transform(building_tags_list)\n",
    "    \n",
    "#     # Getting shape of matrix\n",
    "#     x, y = X.shape\n",
    "    \n",
    "#     # Converting sparse matrix to numpy array\n",
    "#     X = X.toarray()\n",
    "    \n",
    "#     # Finding the Optimal number of clusters\n",
    "#     optimalK = OptimalK(n_jobs=4, parallel_backend='multiprocessing')\n",
    "#     n_clusters = optimalK(X = X, cluster_array=np.arange(1, x))\n",
    "    \n",
    "#     # Converting numpy array to rdd\n",
    "#     rdd = sc.parallelize(X)\n",
    "    \n",
    "#     # Converting rdd to dense vectors\n",
    "#     trainingData = rdd.map(lambda line: Vectors.dense([float(x) for x in line]))\n",
    "        \n",
    "#     # Setting up training queue\n",
    "#     trainingQueue = [trainingData]\n",
    "    \n",
    "#     # Setting up training stream\n",
    "#     trainingStream = ssc.queueStream(trainingQueue)\n",
    "    \n",
    "#     # We create a model with random clusters and specify the optimal number of clusters to find\n",
    "#     model = StreamingKMeans(k=n_clusters, decayFactor=0.8).setRandomCenters(y, 1.0, 0)\n",
    "    \n",
    "#     # Making a window of 10,000 on the training data\n",
    "#     trainingWindow = trainingStream.window(10000, 1)\n",
    "    \n",
    "#     # Now training the model on training data\n",
    "#     model.trainOn(trainingWindow)\n",
    "    \n",
    "#     # Predicting the values on the training data\n",
    "#     result = model.predictOn(trainingStream)\n",
    "    \n",
    "#     # Saving function\n",
    "#     def save_file(rdd):\n",
    "#         window = rdd.zip(trainingData.map(lambda x: x.toArray().tolist()))\n",
    "#         window.coalesce(1).saveAsTextFile('Cluster Predictions/' + file)\n",
    "        \n",
    "#     result.foreachRDD(save_file)\n",
    "    \n",
    "#     ssc.start()\n",
    "#     ssc.stop(stopSparkContext = False, stopGraceFully = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae192544-56bb-46e6-b3aa-84c1c8d9d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over all the building tag files to implement Streaming Kmeans Clustering\n",
    "for i in range(len(bt_file_names)):\n",
    "        \n",
    "    building_tags_file_names = os.listdir(\"Building Tags/\" + bt_file_names[i])\n",
    "\n",
    "    for csv_file in building_tags_file_names:\n",
    "        if csv_file.endswith(\".csv\"):\n",
    "            building_tags = spark.read.options(header='True').csv(\"Building Tags/\" + bt_file_names[i] + \"/\" + csv_file)\n",
    "                        \n",
    "    # Dropping the lcc column as we don't need it now\n",
    "    building_tags = building_tags.drop(\"lcc\")\n",
    "    \n",
    "    # Converting df to rdd returning vertex_tag_label\n",
    "    building_tags_rdd = building_tags.rdd.map(lambda x: tags_mapper(x))\n",
    "\n",
    "    # Converting rdd to list\n",
    "    building_tags_list = building_tags_rdd.collect()\n",
    "    \n",
    "    # Vectorizing the tags\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(building_tags_list)\n",
    "        \n",
    "    # Getting shape of matrix\n",
    "    x, y = X.shape\n",
    "    \n",
    "    # Converting sparse matrix to numpy array\n",
    "    X = X.toarray()\n",
    "    \n",
    "    # Initializing number of cluster tries\n",
    "    tries = min(x, 20)\n",
    "    \n",
    "    # Finding the Optimal number of clusters\n",
    "    optimalK = OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "    if y <= 1000:\n",
    "        n_clusters = optimalK(X = X, cluster_array=np.arange(1, tries))\n",
    "    else:\n",
    "        n_clusters = 20\n",
    "            \n",
    "    # Applying K means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "        \n",
    "    # Saving the clustered results    \n",
    "    with open('Cluster Predictions/' + bt_file_names[i] + '.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['vertex_tag_label', 'Cluster'])\n",
    "        writer.writerows(zip(building_tags_list, kmeans.labels_.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf023e3-5cba-468e-a4c1-f9854339a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd933c2c-c3a3-47d2-b411-2e41a350196d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 3 in Python 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
